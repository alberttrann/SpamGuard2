import streamlit as st
import requests
import json
import time
import pandas as pd

# --- Configuration ---
API_BASE_URL = "http://127.0.0.1:8000"

# --- Page Setup ---
st.set_page_config(page_title="SpamGuard AI", page_icon="üõ°Ô∏è", layout="wide")
st.title("üõ°Ô∏è SpamGuard AI: An Adaptive Spam Filtering System")

# --- Initialize Session State ---
if 'last_classified_message' not in st.session_state:
    st.session_state.last_classified_message = None
if 'generating' not in st.session_state:
    st.session_state.generating = False
if 'generation_type' not in st.session_state:
    st.session_state.generation_type = None # Will store 'spam', 'ham', or None

# --- API Functions ---
def classify_message(message):
    try:
        response = requests.post(f"{API_BASE_URL}/classify", json={"text": message})
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"API Error: Could not classify message. Details: {e}")
        return None

def send_feedback(message, correct_label):
    try:
        payload = {"message": message, "correct_label": correct_label}
        response = requests.post(f"{API_BASE_URL}/feedback", json=payload)
        response.raise_for_status()
        st.toast(f"‚úÖ Feedback sent! Model will learn from this.")
    except requests.exceptions.RequestException as e:
        st.error(f"API Error: Could not send feedback. Details: {e}")

def retrain_model():
    try:
        response = requests.post(f"{API_BASE_URL}/retrain")
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"API Error: Could not trigger retraining. Details: {e}")
        return None

# --- UI Sections ---

# SECTION 1: CLASSIFICATION
st.header("1. Real-time Classification")
message_input = st.text_area("Enter a message to analyze:", height=100, key="msg_input")

if st.button("Classify Message", use_container_width=True):
    if message_input:
        with st.spinner("Analyzing..."):
            result = classify_message(message_input)
        if result:
            st.session_state.last_classified_message = {"message": message_input, **result}
    else:
        st.warning("Please enter a message.")

# Display classification result and feedback options
if st.session_state.last_classified_message:
    res = st.session_state.last_classified_message
    st.subheader("Analysis Result")
    
    pred = res['prediction']
    conf = res['confidence']
    model = res['model']
    
    color = "error" if pred == "spam" else "success"
    icon = "üö®" if pred == "spam" else "‚úÖ"
    st.markdown(f"### <span style='color:{'red' if color=='error' else 'green'};'>{icon} Prediction: **{pred.upper()}**</span>", unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    col1.metric("Confidence", f"{conf:.2%}")
    col2.metric("Model Used", model)

    if res.get('evidence'):
        with st.expander("üí° See Why (Explainable AI)"):
            st.write("The model found these messages in its database to be most similar:")
            for item in res['evidence']:
                st.info(f"**{item['label'].upper()}** (Similarity: {item['similarity_score']:.3f}):\n\n_{item['similar_message']}_")

    st.subheader("Was this correct?")
    fb_col1, fb_col2, fb_col3 = st.columns([1,1,5])
    if fb_col1.button("‚úîÔ∏è Yes, it's correct!"):
        send_feedback(res['message'], res['prediction'])
    if fb_col2.button("‚ùå No, it's wrong!"):
        wrong_label = "ham" if pred == "spam" else "spam"
        send_feedback(res['message'], wrong_label)

st.divider()

# SECTION 2: MODEL TRAINING & ANALYTICS
st.header("2. Model Performance & Learning")
analytics_col, training_col = st.columns(2)

with analytics_col:
    st.subheader("Dataset Analytics")
    if st.button("Refresh Analytics"):
        try:
            stats = requests.get(f"{API_BASE_URL}/analytics").json()
            total_base = stats['base_ham_count'] + stats['base_spam_count']
            total_new = stats['new_ham_count'] + stats['new_spam_count']

            st.metric("Total Messages in Core Dataset", f"{total_base:,}")
            st.metric("New Messages Ready for Training", f"{total_new:,}", help=f"User: {stats['user_contribution']}, LLM: {stats['llm_contribution']}")
            
            chart_data = pd.DataFrame({
                "Type": ["Ham", "Spam"],
                "Core Dataset": [stats['base_ham_count'], stats['base_spam_count']],
                "New Data": [stats['new_ham_count'], stats['new_spam_count']]
            }).set_index('Type')
            st.bar_chart(chart_data)

        except (requests.exceptions.RequestException, json.JSONDecodeError):
            st.error("Could not fetch analytics.")

with training_col:
    st.subheader("Continuous Learning")
    st.write("Add the new data from user feedback and LLM generation to the main dataset and retrain the models.")
    if st.button("Retrain Model with New Data", use_container_width=True, type="primary"):
        with st.spinner("Retraining in progress... This may take a few minutes."):
            retrain_result = retrain_model()
            if retrain_result:
                if retrain_result['status'] == 'success':
                    st.success(retrain_result['message'])
                    st.balloons()
                else:
                    st.info(retrain_result['message'])

st.divider()


# SECTION 3: LLM DATA DISTILLATION
st.header("3. Augment Data with an LLM")
st.write("Automatically generate new training data to improve the model's performance and help it adapt to new types of messages.")

with st.container():
    use_llm = st.toggle("Enable LLM Data Generation Controls")

    if use_llm:
        # --- FIX: Use a dictionary to map display names to API provider names ---
        PROVIDER_MAP = {
            "Ollama (Local)": "ollama",
            "LM Studio (Local)": "lmstudio",
            "OpenRouter (Cloud)": "openrouter"
        }
        
        provider_display_name = st.radio(
            "Choose LLM Provider",
            PROVIDER_MAP.keys(), # Show user-friendly names
            horizontal=True,
            key="provider"
        )
        
        # Get the correct API provider name from the map
        provider_api_name = PROVIDER_MAP[provider_display_name]
        
        # Updated logic to handle inputs for all three providers
        if provider_api_name == "ollama":
            model_id = st.text_input("Ollama Model ID", "llama3", key="ollama_model")
            api_key = ""
        elif provider_api_name == "lmstudio":
            model_id = st.text_input(
                "Model Identifier (usually ignored)", "local-model", key="lmstudio_model",
                help="LM Studio typically uses the model loaded in the UI, so this can be left as is."
            )
            api_key = ""
        else: # openrouter
            model_id = st.text_input("OpenRouter Model ID", "mistralai/mistral-7b-instruct:free", key="or_model")
            api_key = st.text_input("OpenRouter API Key", type="password", key="or_api_key")

        st.subheader("Generation Controls")
        gen_col1, gen_col2, gen_col3 = st.columns(3)

        if gen_col1.button("ü§ñ Start Continuous Generation (Random)", use_container_width=True, disabled=st.session_state.generating):
            st.session_state.generating = True
            st.session_state.generation_type = None
            st.rerun()

        if gen_col2.button("üö® Start Continuous Generation (SPAM)", use_container_width=True, disabled=st.session_state.generating):
            st.session_state.generating = True
            st.session_state.generation_type = "spam"
            st.rerun()
        
        if gen_col3.button("‚úÖ Start Continuous Generation (HAM)", use_container_width=True, disabled=st.session_state.generating):
            st.session_state.generating = True
            st.session_state.generation_type = "ham"
            st.rerun()

        if st.session_state.generating:
            st.info("LLM is continuously generating data. Press 'Stop Generating' to halt the process.")
            if st.button("‚èπÔ∏è Stop Generating", use_container_width=True, type="primary"):
                st.session_state.generating = False
                st.session_state.generation_type = None
                st.rerun()

            status_box = st.empty()
            
            # --- FIX: Use the correct provider_api_name in the payload ---
            payload = {
                "provider": provider_api_name,
                "model": model_id,
                "api_key": api_key,
                "label_to_generate": st.session_state.generation_type
            }

            try:
                with requests.post(f"{API_BASE_URL}/generate_data", json=payload, stream=True, timeout=300) as r:
                    r.raise_for_status()
                    for chunk in r.iter_content(chunk_size=None, decode_unicode=True):
                        if not st.session_state.generating:
                            status_box.warning("Generation stopped by user.")
                            break
                        
                        if chunk.startswith("data:"):
                            status_message = chunk.split("data:", 1)[1].strip()
                            status_box.write(f"ü§ñ LLM Status: {status_message}")

            except requests.exceptions.RequestException as e:
                status_box.error(f"Connection lost or error during generation: {e}")
            
            if not st.session_state.generating:
                st.success("Process halted.")
            else:
                st.session_state.generating = False
                st.warning("Stream closed unexpectedly. Please start generation again.")